# Disaster Response Pipeline Project

This repo contains scripts for a machine learning (ML) workflow that categorizes messages into disaster respones categories. First, we clean and process the data with a extract, transform, load ([ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load)) pipeline followed by training a machine learning model to categorize the messages. Finally, we present a simple web app that runs the model and displays some visualizations.

This application aims to help quickly identify signals of distress or disaster from text messages. It will categorize the message so that it is clear which disaster relief agency should act on it.

This work was carried out as homework as part of the [Udacity Data Scientist Nanodegree](https://www.udacity.com/course/data-scientist-nanodegree--nd025).

### Setup:
Setup Python environment

```bash
# Recommended to setup a conda environment
conda create -n disaster-pipeline python=3.7 
conda activate disaster-pipeline
# Install requirements
pip install -r requirements.txt 
```

### Instructions:
1. Run the following commands in the project's root directory to set up your database and model.

    - To run ETL pipeline that cleans data and stores in database
        `python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db`
    - To run ML pipeline that trains classifier and saves
        `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`
    - To run ML pipeline that trains the classifier and saves with parameter grid search 
        `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl --grid_search`

2. Run the following command in the app's directory to run your web app. (`cd app`)
    `python run.py ../data/DisasterResponse.db ../models/classifier.pkl`

3. Go to http://0.0.0.0:3001/

### Files

```
disaster-response-pipeline
├── app
│   ├── templates
│   │   ├── go.html                             # classification result page of web app
│   │   └── master.html                         # main page of web app
│   └── run.py                                  # Flash file that runs the web app
│
├── data
│   ├── disaster_categories.csv                 # Categories data to process
│   ├── disaster_messages.csv                   # Messages data to process
│   ├── 1 - ETL Pipeline Preparation.ipynb      # Rudimentary version of `process_data.py` in notebook form
│   ├── 2 - Exploratory Data Analysis           # Analysis of data produced by `process_data.py`
│   ├── <InsertDatabaseName>.db                 # Database to save cleaned data to (generated by process_data.py)
│   └── process_data.py                         # Script to process data
│
├── models
│   ├── 1 - ML Pipeline Preparation             # Rudimentary version of `train_classifier.py` in notebook form
│   ├── <InsertModelName>.pkl                   # Saved model (generated by train_classifier.py)
│   └── train_classifier.py                     # Train ML model and save 
│
├── requirements.txt                            # Pip requirements
└── README.md                                   # Documentation
```